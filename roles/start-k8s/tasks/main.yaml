- name: Start k8s
  shell:
    cmd: |
      set -x
      set -e
      set -o pipefail

      export API_HOST_IP=$(ifconfig | awk '/^docker0/ {getline; print $2}' | awk -F ':' '{print $2}')
      export KUBELET_HOST="0.0.0.0"
      export ALLOW_SECURITY_CONTEXT=true
      export ENABLE_CRI=false
      export ENABLE_HOSTPATH_PROVISIONER=true
      export ENABLE_SINGLE_CA_SIGNER=true
      export KUBE_ENABLE_CLUSTER_DNS=false
      export LOG_LEVEL=4
      # We want to use the openstack cloud provider
      export CLOUD_PROVIDER=openstack
      # We want to run a separate cloud-controller-manager for openstack
      export EXTERNAL_CLOUD_PROVIDER=true
      # DO NOT change the location of the cloud-config file. It is important for the old cinder provider to work
      export CLOUD_CONFIG=/etc/kubernetes/cloud-config
      # Specify the OCCM binary
      export EXTERNAL_CLOUD_PROVIDER_BINARY=./openstack-cloud-controller-manager

      # location of where the kubernetes processes log their output
      mkdir -p '{{ ansible_user_dir }}/workspace/logs/kubernetes'
      export LOG_DIR='{{ ansible_user_dir }}/workspace/logs/kubernetes'
      # We need this for one of the conformance tests
      export ALLOW_PRIVILEGED=true
      # Just kick off all the processes and drop down to the command line
      export ENABLE_DAEMON=true
      export HOSTNAME_OVERRIDE=$(curl http://169.254.169.254/openstack/latest/meta_data.json | python -c "import sys, json; print json.load(sys.stdin)['name']")
      export MAX_TIME_FOR_URL_API_SERVER=5

      # -E preserves the current env vars, but we need to special case PATH
      sudo -E PATH=$PATH SHELLOPTS=$SHELLOPTS "$GOPATH/src/k8s.io/kubernetes/hack/local-up-cluster.sh" -O

      # Start k8s plugin
      case '{{ zuul.job }}' in
          cloud-provider-openstack-acceptance-test-csi-cinder)
              ;;
          cloud-provider-openstack-acceptance-test-k8s-cinder)
              nohup ./cinder-provisioner \
                  --cloud-config /etc/kubernetes/cloud-config \
                  --kubeconfig /var/run/kubernetes/admin.kubeconfig \
                  --id cinder \
                  > '{{ ansible_user_dir }}/workspace/logs/kubernetes/cinder-provisioner.log' 2>&1 &
              ;;
          cloud-provider-openstack-acceptance-test-keystone-authentication-authorization)
              LOG_DIR='{{ ansible_user_dir }}/workspace/logs/kubernetes'
              nohup ./k8s-keystone-auth \
                    --tls-cert-file /var/run/kubernetes/serving-kube-apiserver.crt \
                    --tls-private-key-file /var/run/kubernetes/serving-kube-apiserver.key \
                    --keystone-policy-file /etc/kubernetes/policy.json \
                    --log-dir="$LOG_DIR" \
                    --v=10 \
                    --keystone-url "$OS_AUTH_URL" \
                    > "$LOG_DIR/keystone-auth.log" 2>&1 &
              ;;
          *)
              echo 'There is no plugin of this job "{{ zuul.job }}", exit 1'
              exit 1
              ;;
      esac

      # set up the config we need for kubectl to work
      "$kubectl" config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt
      "$kubectl" config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt
      "$kubectl" config set-context local --cluster=local --user=myself
      "$kubectl" config use-context local

      # Hack for RBAC for all for the new cloud-controller process, we need to do better than this
      "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
      "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
      "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
      "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
      "$kubectl" create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
      "$kubectl" create clusterrolebinding --user system:kube-controller-manager kube-system-cluster-admin-6 --clusterrole cluster-admin
    executable: /bin/bash
    chdir: '{{ zuul.project.src_dir }}'
  environment: '{{ golang_env }}'
