- hosts: all
  name: workaround to set up hostname
  become: yes

  tasks:
    - name: Make sure the utility packages are installed
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: yes
      vars:
        packages:
          - jq

    # Nova VMs created in citynetwork use ubuntu as the default hostname, should be changed to the VM name so that
    # the kubernetes node could be initialized.
    - name: Get instance hostname from Nova metadata service
      shell:
        executable: /bin/bash
        cmd: |
          nova_hostname=$(curl -sS http://169.254.169.254/openstack/latest/meta_data.json | jq -r '.hostname')
          echo ${nova_hostname%%.novalocal}
      register: hostname_output

    - name: Set hostname
      hostname:
        name: "{{ hostname_output.stdout }}"

    - name: hostname in /etc/hosts
      lineinfile:
        dest: /etc/hosts
        regexp: '^127\.0\.1\.1'
        line: '127.0.1.1 {{ hostname_output.stdout }}'
        state: present

- hosts: all
  roles:
    - config-golang
    - export-cloud-openrc
    - role: install-k8s
      vars:
        customize_docker_version: '20.10'
  become: yes
  tasks:
    - name: Prepare base alpine image
      shell:
         cmd: |
           docker login -u ap-southeast-3@{{ swr.ak }} -p {{ swr.sk }} swr.ap-southeast-3.myhuaweicloud.com
           docker pull swr.ap-southeast-3.myhuaweicloud.com/openlab/alpine:3.11
           docker tag swr.ap-southeast-3.myhuaweicloud.com/openlab/alpine:3.11 alpine:3.11
           docker logout swr.ap-southeast-3.myhuaweicloud.com
      no_log: yes

    - name: Run csi cinder acceptance tests with cloud-provider-openstack
      shell:
        cmd: |
          set -x
          set -e
          set -o pipefail

          export ARCH=${ARCH:-amd64}

          # Build openstack-cloud-controller-manager binary
          export BUILD_CMDS=openstack-cloud-controller-manager
          make build

          # Create cloud-config
          mkdir -p /etc/kubernetes/
          cat << EOF >> /etc/kubernetes/cloud-config
          [Global]
          domain-name = $OS_USER_DOMAIN_NAME
          tenant-id = $OS_PROJECT_ID
          auth-url = $OS_AUTH_URL
          password = $OS_PASSWORD
          username = $OS_USERNAME
          region = $OS_REGION_NAME
          [BlockStorage]
          bs-version = v3
          ignore-volume-az = yes
          EOF

          export ARTIFACTS=/home/zuul/workspace/logs/_artifacts
          export API_HOST_IP=$(ip route get 1.1.1.1 | awk '{print $7}')
          export KUBELET_HOST="0.0.0.0"
          export ALLOW_SECURITY_CONTEXT=true
          export ENABLE_CRI=false
          export ENABLE_HOSTPATH_PROVISIONER=true
          export ENABLE_SINGLE_CA_SIGNER=true
          export KUBE_ENABLE_CLUSTER_DNS=false
          export LOG_LEVEL=4
          # We want to use the openstack cloud provider
          export CLOUD_PROVIDER=openstack
          # We want to run a separate cloud-controller-manager for openstack
          export EXTERNAL_CLOUD_PROVIDER=true
          # DO NOT change the location of the cloud-config file. It is important for the old cinder provider to work
          export CLOUD_CONFIG=/etc/kubernetes/cloud-config
          # Specify the OCCM binary
          export EXTERNAL_CLOUD_PROVIDER_BINARY="$PWD/openstack-cloud-controller-manager"

          # location of where the kubernetes processes log their output
          mkdir -p '{{ k8s_log_dir }}'
          export LOG_DIR='{{ k8s_log_dir }}'
          # We need this for one of the conformance tests
          export ALLOW_PRIVILEGED=true
          # Just kick off all the processes and drop down to the command line
          export ENABLE_DAEMON=true
          if [ -d /mnt/config/openstack ]; then
              export HOSTNAME_OVERRIDE=$(hostname)
          else
              export HOSTNAME_OVERRIDE=$(curl http://169.254.169.254/openstack/latest/meta_data.json | python -c "import sys, json; print json.load(sys.stdin)['name']")
          fi
          export MAX_TIME_FOR_URL_API_SERVER=5

          # Requirements to deploy the csi cinder driver
          export RUNTIME_CONFIG="storage.k8s.io/v1alpha1=true"

          # -E preserves the current env vars, but we need to special case PATH
          # Must run local-up-cluster.sh under kubernetes root directory
          pushd '{{ k8s_src_dir }}'
          sudo -E PATH=$PATH SHELLOPTS=$SHELLOPTS ./hack/local-up-cluster.sh -O
          popd

          # set up the config we need for kubectl to work
          '{{ kubectl }}' config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt
          '{{ kubectl }}' config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt
          '{{ kubectl }}' config set-context local --cluster=local --user=myself
          '{{ kubectl }}' config use-context local
          # Hack for RBAC for all for the new cloud-controller process, we need to do better than this
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin

          # Get current branch name, convert to image tag, e.g. release-1.18 to v1.18.0
          branch="{{ zuul.branch }}"
          if [[ $branch == "master" ]]; then
              version=latest
          else
              version="v${branch##release-}.0"
          fi
          export VERSION=$version
          make image-csi-plugin
          if [[ -z "$(docker images -q k8scloudprovider/cinder-csi-plugin:$VERSION 2> /dev/null)" ]]; then
              # add tag without architecture so we don't need to push manifests
              docker image tag k8scloudprovider/cinder-csi-plugin-$ARCH:$VERSION k8scloudprovider/cinder-csi-plugin:$VERSION
          fi

          # Replace custom cloud config
          {
              cloud_cfg=$(base64 -w 0 ${CLOUD_CONFIG})
              sed "s/cloud\.conf:.*$/cloud.conf: $cloud_cfg/g" -i manifests/cinder-csi-plugin/csi-secret-cinderplugin.yaml
          } > /dev/null 2>&1

          # Enable services
          '{{ kubectl }}' create -f manifests/cinder-csi-plugin
          sleep 5
          # If services up
          if timeout 300 bash -c '
              while :
              do
                  {{ kubectl }} get pod --all-namespaces | sed "1d" | awk '\''$4 != "Running" {err = 1} END {exit err}'\'' && break
                  sleep 1
              done
              '
          then
              echo 'Run services successful'
              '{{ kubectl }}' get pod --all-namespaces
          else
              echo 'Run services failed'
              '{{ kubectl }}' describe pod --all-namespaces
              exit 1
          fi

          # Make test
          '{{ kubectl }}' create -f examples/cinder-csi-plugin/nginx.yaml
          # If test passed
          if timeout 300 bash -c '
              while :
              do
                  [[ $({{ kubectl }} describe pod nginx | awk "/^Status:/ {print \$2}") == Running ]] && break
                  sleep 1
              done
              '
          then
              echo 'Run test successful'
              '{{ kubectl }}' get pod
          else
              echo 'Run test failed'

              mkdir -p "${ARTIFACTS}/logs"
              kubectl get clusters,openstackclusters,machines,openstackmachines,kubeadmconfigs,machinedeployments,openstackmachinetemplates,kubeadmconfigtemplates,machinesets --all-namespaces -o yaml > "${ARTIFACTS}/logs/all_object_info.txt" || true

              '{{ kubectl }}' get pod
              '{{ kubectl }}' describe pod nginx
              echo '-------------logs for controller plugin csi-attacher----------'
              '{{ kubectl }}' logs csi-cinder-controllerplugin-0 -c csi-attacher
              echo '-------------logs for controller plugin csi-provisioner----------'
              '{{ kubectl }}' logs csi-cinder-controllerplugin-0 -c csi-provisioner
              echo '-------------logs for controller plugin csi-snapshotter----------'
              '{{ kubectl }}' logs csi-cinder-controllerplugin-0 -c csi-snapshotter
              echo '-------------logs for controller plugin csi-resizer----------'
              '{{ kubectl }}' logs csi-cinder-controllerplugin-0 -c csi-resizer
              echo '-------------logs for controller plugin cinder-csi-plugin----------'
              '{{ kubectl }}' logs csi-cinder-controllerplugin-0 -c cinder-csi-plugin

              '{{ kubectl }}' describe pvc csi-pvc-cinderplugin
              exit 1
          fi
        executable: /bin/bash
        chdir: '{{ k8s_os_provider_src_dir }}'
      environment: '{{ global_env }}'
