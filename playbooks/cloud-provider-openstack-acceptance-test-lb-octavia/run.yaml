- hosts: all
  become: yes
  roles:
    - config-golang
    - export-cloud-openrc
    - install-k8s
  tasks:
    - name: Run lbass octavia acceptance tests with cloud-provider-openstack
      shell:
        cmd: |
          set -x
          set -e
          set -o pipefail

          # Build cloud-provider-openstack binaries
          make build

          apt-get install python-pip -y
          pip install -U python-openstackclient python-octaviaclient python-neutronclient

          # Create cloud-config
          mkdir -p /etc/kubernetes/
          cat << EOF >> /etc/kubernetes/cloud-config
          [Global]
          domain-name = $OS_USER_DOMAIN_NAME
          tenant-id = $OS_PROJECT_ID
          auth-url = $OS_AUTH_URL
          password = $OS_PASSWORD
          username = $OS_USERNAME
          region = $OS_REGION_NAME
          [LoadBalancer]
          use-octavia = true
          floating-network-id = $(openstack network list --external -f value -c ID | head -n 1)
          subnet-id = $(openstack network list --internal -f value -c Subnets | head -n 1 | cut -d "," -f1)
          [BlockStorage]
          bs-version = v3
          ignore-volume-az = yes
          EOF

          export API_HOST_IP=$(ip route get 1.1.1.1 | awk '{print $7}')
          export KUBELET_HOST="0.0.0.0"
          export ALLOW_SECURITY_CONTEXT=true
          export ENABLE_CRI=false
          export ENABLE_HOSTPATH_PROVISIONER=true
          export ENABLE_SINGLE_CA_SIGNER=true
          export KUBE_ENABLE_CLUSTER_DNS=false
          export LOG_LEVEL=4
          # We want to use the openstack cloud provider
          export CLOUD_PROVIDER=openstack
          # We want to run a separate cloud-controller-manager for openstack
          export EXTERNAL_CLOUD_PROVIDER=true
          # DO NOT change the location of the cloud-config file. It is important for the old cinder provider to work
          export CLOUD_CONFIG=/etc/kubernetes/cloud-config
          # Specify the OCCM binary
          export EXTERNAL_CLOUD_PROVIDER_BINARY="$PWD/openstack-cloud-controller-manager"

          # location of where the kubernetes processes log their output
          mkdir -p '{{ k8s_log_dir }}'
          export LOG_DIR='{{ k8s_log_dir }}'
          # We need this for one of the conformance tests
          export ALLOW_PRIVILEGED=true
          # Just kick off all the processes and drop down to the command line
          export ENABLE_DAEMON=true
          if [ -d /mnt/config/openstack ]; then
              export HOSTNAME_OVERRIDE=$(hostname)
          else
              export HOSTNAME_OVERRIDE=$(curl http://169.254.169.254/openstack/latest/meta_data.json | python -c "import sys, json; print json.load(sys.stdin)['name']")
          fi
          export MAX_TIME_FOR_URL_API_SERVER=5

          # -E preserves the current env vars, but we need to special case PATH
          # Must run local-up-cluster.sh under kubernetes root directory
          pushd '{{ k8s_src_dir }}'
          sudo -E PATH=$PATH SHELLOPTS=$SHELLOPTS ./hack/local-up-cluster.sh -O
          popd

          # set up the config we need for kubectl to work
          '{{ kubectl }}' config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt
          '{{ kubectl }}' config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt
          '{{ kubectl }}' config set-context local --cluster=local --user=myself
          '{{ kubectl }}' config use-context local
          # Hack for RBAC for all for the new cloud-controller process, we need to do better than this
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
          '{{ kubectl }}' create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin

          # Run test
          for test_case in internal external
          do
            test_file="examples/loadbalancers/${test_case}-http-nginx.yaml"
            service_name="${test_case}-http-nginx-service"
            # Delete fake floating-network-id to use the default one in cloud config
            sed -i '/loadbalancer.openstack.org/d' "$test_file"
            '{{ kubectl }}' create -f "$test_file"

            if ! service_name="$service_name" timeout 600 bash -c '
                while :
                do
                    [[ -n $({{ kubectl }} describe service "$service_name" | awk "/LoadBalancer Ingress/ {print \$3}") ]] && break
                    sleep 1
                done
                '
            then
                echo "Timed out to wait for $test_case loadbalancer services deployment!"
                '{{ kubectl }}' describe pods
                '{{ kubectl }}' describe services
                exit 1
            fi

            ingress_ip=$('{{ kubectl }}' describe service "$service_name" | awk "/LoadBalancer Ingress/ {print \$3}")
            if curl --retry 5 --retry-max-time 30 "http://$ingress_ip" | grep 'Welcome to nginx'
            then
                echo "$test_case lb services launched sucessfully!"
            else
                echo "$test_case lb services launched failed!"
                exit 1
            fi
          done
        executable: /bin/bash
        chdir: '{{ k8s_os_provider_src_dir }}'
      environment: '{{ global_env }}'
